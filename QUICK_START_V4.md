# Quick Start - Model V4 Training

## ğŸš€ Quick Start

### 1. Install Dependencies
```bash
pip install optuna catboost lightgbm xgboost scikit-learn pandas numpy
```

### 2. Run Training
```bash
python train_model_v4_optimized.py
```

### 3. Expected Results
- **Target**: RÂ² â‰¥ 0.85 (85%+ accuracy)
- **Training Time**: ~3-5 hours
- **Output**: `models/best_model_v4.pkl`

---

## âš¡ Fast Training (Testing)

For quick testing, reduce optimization trials:

```python
# Edit train_model_v4_optimized.py, change:
OPTUNA_TRIALS = 10  # Instead of 100
```

This reduces training time to ~30-60 minutes.

---

## ğŸ“Š What Gets Trained

1. **XGBoost** (Optuna optimized)
2. **CatBoost** (Optuna optimized)
3. **LightGBM**
4. **Ensemble (XGBoost + CatBoost)** - 60/40
5. **Ensemble (3 Models)** - 40/30/30
6. **Stacking Ensemble** - Ridge meta-learner

**Best model automatically selected!**

---

## ğŸ¯ Features Added

- âœ… Luxury brand indicators
- âœ… Age-based depreciation curves
- âœ… Mileage per year analysis
- âœ… Market segmentation
- âœ… Condition numeric encoding
- âœ… Popular model flags
- âœ… Interaction features
- âœ… Price range segmentation

---

## ğŸ“ Output Files

- `models/best_model_v4.pkl` - Best model
- `models/xgboost_model_v4.pkl` - XGBoost
- `models/catboost_model_v4.pkl` - CatBoost
- `models/lightgbm_model_v4.pkl` - LightGBM
- `models/model_v4_info.json` - Model info

---

## âœ… Success Criteria

- RÂ² â‰¥ 0.85 âœ…
- RMSE < $5,000 âœ…
- MAE < $3,000 âœ…
- MAPE < 10% âœ…

---

## ğŸ” Monitor Progress

```bash
# Watch log file
tail -f model_training_v4.log

# Or check console output
```

---

## ğŸ‰ Ready to Train!

Just run:
```bash
python train_model_v4_optimized.py
```

The script will:
1. âœ… Create advanced features
2. âœ… Optimize hyperparameters (100 trials)
3. âœ… Train all models
4. âœ… Create ensembles
5. âœ… Train price range models
6. âœ… Save best model
7. âœ… Report results

**Good luck! ğŸš€**
